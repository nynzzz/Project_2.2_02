{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["NRlnSoIKhqa_"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"d4d614fe7a604e0e9a060d3c263ab113":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_169c496b52f8402396e034276a51c2c0","IPY_MODEL_29ee746d6af84b1fbded1bce6ab8af56","IPY_MODEL_2b566de9f8f34a45a6782f8649421399"],"layout":"IPY_MODEL_f7d47b1ea1e8491f86bdf861b5e6550b"}},"169c496b52f8402396e034276a51c2c0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a94c167b0d8948aea914ece7d92c45b8","placeholder":"​","style":"IPY_MODEL_22389ff6a8ec4314a9fa6a6c56b953be","value":"Downloading pytorch_model.bin: 100%"}},"29ee746d6af84b1fbded1bce6ab8af56":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_814ee242838f4b71ad850ba0c75817ab","max":891691430,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d1a7a2f6db1f4db884a35f9702625011","value":891691430}},"2b566de9f8f34a45a6782f8649421399":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_10793874d0e0460f8fa1b30b457e16a9","placeholder":"​","style":"IPY_MODEL_7a4306f5393f44ff87117c03c8c33aef","value":" 892M/892M [00:10&lt;00:00, 104MB/s]"}},"f7d47b1ea1e8491f86bdf861b5e6550b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a94c167b0d8948aea914ece7d92c45b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22389ff6a8ec4314a9fa6a6c56b953be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"814ee242838f4b71ad850ba0c75817ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1a7a2f6db1f4db884a35f9702625011":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"10793874d0e0460f8fa1b30b457e16a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a4306f5393f44ff87117c03c8c33aef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"53ddbbb2b80444f6b4fc2324b8008d10":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_63937ed18e474b91a2d6ce195aae2a56","IPY_MODEL_7eb710c237c14389a6ce3ad5db5b4ac1","IPY_MODEL_d885d79164fa4117b0cbf263553acee7"],"layout":"IPY_MODEL_3d010ad3e8b44509bef5585be9c04eee"}},"63937ed18e474b91a2d6ce195aae2a56":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_783ba23c60ab44c285be7ea62d81c378","placeholder":"​","style":"IPY_MODEL_765d40016f954074b71ddf241e34590c","value":"Downloading (…)neration_config.json: 100%"}},"7eb710c237c14389a6ce3ad5db5b4ac1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_88924571ac76424eaebc3e61711ff92e","max":147,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d53c5dd6d53542b5add85a553b4a6f2f","value":147}},"d885d79164fa4117b0cbf263553acee7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0feddfd635047e9a3be93386022b188","placeholder":"​","style":"IPY_MODEL_763ea763a28a40ef918eac78abdbe539","value":" 147/147 [00:00&lt;00:00, 9.10kB/s]"}},"3d010ad3e8b44509bef5585be9c04eee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"783ba23c60ab44c285be7ea62d81c378":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"765d40016f954074b71ddf241e34590c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"88924571ac76424eaebc3e61711ff92e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d53c5dd6d53542b5add85a553b4a6f2f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c0feddfd635047e9a3be93386022b188":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"763ea763a28a40ef918eac78abdbe539":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["!pip install bert_score\n","!pip install datsets transformers[sentencepiece]\n","!pip install sentencepiece"],"metadata":{"id":"N__dQGy_haxn"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O_21IfUifite"},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n","from bert_score import score\n","import numpy as np\n","import pandas as pd\n","from transformers import T5Tokenizer, T5ForConditionalGeneration\n","from torch import cuda"]},{"cell_type":"code","source":["device = 'cuda' if cuda.is_available() else 'cpu'"],"metadata":{"id":"KFI-TDJvySEV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CustomDataset(Dataset): # https://www.learnpytorch.io/04_pytorch_custom_datasets/\n","\n","    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n","        self.tokenizer = tokenizer\n","        self.data = dataframe\n","        self.source_len = source_len\n","        self.summ_len = summ_len\n","        self.original = self.data.src\n","        self.summary = self.data.tgt\n","\n","    def __len__(self):\n","        return len(self.original)\n","\n","    def __getitem__(self, index):\n","        summary = str(self.summary[index])\n","        summary = ' '.join(summary.split())\n","\n","        original = str(self.original[index])\n","        original = ' '.join(original.split())\n","\n","        source = self.tokenizer.batch_encode_plus([summary], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n","        target = self.tokenizer.batch_encode_plus([original], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n","\n","        source_ids = source['input_ids'].squeeze()\n","        source_mask = source['attention_mask'].squeeze()\n","        target_ids = target['input_ids'].squeeze()\n","        target_mask = target['attention_mask'].squeeze()\n","\n","        return {\n","            'source_ids': source_ids.to(dtype=torch.long),\n","            'source_mask': source_mask.to(dtype=torch.long),\n","            'target_ids': target_ids.to(dtype=torch.long),\n","            'target_ids_y': target_ids.to(dtype=torch.long)\n","        }"],"metadata":{"id":"KOfO2n7fygRl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(epoch, tokenizer, model, device, loader, optimizer): # https://www.learnpytorch.io/06_pytorch_transfer_learning/\n","    model.train()\n","    for _,data in enumerate(loader, 0):\n","        y = data['target_ids'].to(device, dtype = torch.long)\n","        y_ids = y[:, :-1].contiguous()\n","        lm_labels = y[:, 1:].clone().detach()\n","        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n","        ids = data['source_ids'].to(device, dtype = torch.long)\n","        mask = data['source_mask'].to(device, dtype = torch.long)\n","\n","        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n","        loss = outputs[0]\n","\n","        if _%500==0:\n","            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()"],"metadata":{"id":"VtkRvhh5ypAs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 2\n","MAX_LEN = 1024\n","SUMMARY_LEN = 300\n","\n","tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n","df = pd.read_csv('train_cut.csv',encoding='latin-1')\n","df = df[['src','tgt']]\n","df.src = 'summarize: ' + df.src\n","\n","train_size = 0.8\n","train_dataset=df.sample(frac=train_size).reset_index(drop=True)\n","val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n","\n","training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n","val_set = CustomDataset(val_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n","\n","train_params = {\n","    'batch_size': EPOCHS,\n","    'shuffle': True,\n","    'num_workers': 0\n","    }\n","\n","val_params = {\n","    'batch_size': EPOCHS,\n","    'shuffle': False,\n","    'num_workers': 0\n","    }\n","\n","training_loader = DataLoader(training_set, **train_params)\n","val_loader = DataLoader(val_set, **val_params)\n","\n","model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n","model = model.to(device)\n","\n","optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-4)\n","\n","for epoch in range(EPOCHS):\n","    train(epoch, tokenizer, model, device, training_loader, optimizer)"],"metadata":{"id":"LPz1cu5jytYA","colab":{"base_uri":"https://localhost:8080/","height":292,"referenced_widgets":["d4d614fe7a604e0e9a060d3c263ab113","169c496b52f8402396e034276a51c2c0","29ee746d6af84b1fbded1bce6ab8af56","2b566de9f8f34a45a6782f8649421399","f7d47b1ea1e8491f86bdf861b5e6550b","a94c167b0d8948aea914ece7d92c45b8","22389ff6a8ec4314a9fa6a6c56b953be","814ee242838f4b71ad850ba0c75817ab","d1a7a2f6db1f4db884a35f9702625011","10793874d0e0460f8fa1b30b457e16a9","7a4306f5393f44ff87117c03c8c33aef","53ddbbb2b80444f6b4fc2324b8008d10","63937ed18e474b91a2d6ce195aae2a56","7eb710c237c14389a6ce3ad5db5b4ac1","d885d79164fa4117b0cbf263553acee7","3d010ad3e8b44509bef5585be9c04eee","783ba23c60ab44c285be7ea62d81c378","765d40016f954074b71ddf241e34590c","88924571ac76424eaebc3e61711ff92e","d53c5dd6d53542b5add85a553b4a6f2f","c0feddfd635047e9a3be93386022b188","763ea763a28a40ef918eac78abdbe539"]},"outputId":"7487a2d0-446a-43ba-b874-c80590e89eb1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n","For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n","- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n","- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n","- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4d614fe7a604e0e9a060d3c263ab113"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53ddbbb2b80444f6b4fc2324b8008d10"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 0, Loss:  15.12225341796875\n","Epoch: 1, Loss:  3.128291606903076\n"]}]},{"cell_type":"code","source":["def predict(text, tokenizer, model, device, summary_len=300):\n","    model.eval()\n","    text = 'summarize: ' + text\n","    encoding = tokenizer.encode_plus(text, pad_to_max_length=True, return_tensors=\"pt\")\n","    input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n","\n","    outputs = model.generate(\n","        input_ids=input_ids,\n","        attention_mask=attention_masks,\n","        max_length=summary_len,\n","        num_beams=2,\n","        repetition_penalty=2.5,\n","        length_penalty=1.0,\n","        early_stopping=True\n","    )\n","\n","    output_str = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    return output_str"],"metadata":{"id":"zskGZLJz-SNa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7hO3zpLEsX5o"},"execution_count":null,"outputs":[]}]}